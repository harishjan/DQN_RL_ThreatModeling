{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harishjan/DQN_RL_ThreatModeling/blob/master/DQN_model_cyber_security.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deep reinforcement learning with a scoring framework to enhance the decision making process for security defenses**\n",
        "This project uses dataset based on MITRE ATT&CK\n"
      ],
      "metadata": {
        "id": "zH9NsqQ6-ZMM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LDCaCWviMyp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "from collections import deque\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9xQiOVuiVqU"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "STATE_SIZE = 10  # Number of capabilities\n",
        "ACTION_SIZE = 5  # Number of possible actions (deploy firewall, apply patches, etc.)\n",
        "EPISODES = 1000\n",
        "BATCH_SIZE = 32\n",
        "GAMMA = 0.95  # Discount rate\n",
        "EPSILON = 1.0  # Exploration rate\n",
        "EPSILON_MIN = 0.01\n",
        "EPSILON_DECAY = 0.995\n",
        "LEARNING_RATE = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8FboMP1j3Su"
      },
      "outputs": [],
      "source": [
        "# Improved MITRE ATT&CK dataset generation\n",
        "def create_mitre_attck_dataset(num_samples=1000):\n",
        "    # Define tactics and techniques from the MITRE ATT&CK framework\n",
        "    tactics = ['Initial Access', 'Execution', 'Persistence', 'Privilege Escalation',\n",
        "               'Defense Evasion', 'Credential Access', 'Discovery', 'Lateral Movement',\n",
        "               'Collection', 'Exfiltration']\n",
        "    techniques = ['Phishing', 'Malware', 'Scripting', 'Scheduled Task',\n",
        "                  'Credential Dumping', 'Remote Service', 'System Information',\n",
        "                  'Internal Spearphishing', 'Data Staged', 'Data Encrypted']\n",
        "\n",
        "    # Define capabilities and their effectiveness scores\n",
        "    capabilities = ['Firewall', 'Antivirus', 'IDS', 'DLP', 'SIEM']\n",
        "\n",
        "    data = {\n",
        "        'threat_action': [random.choice(techniques) for _ in range(num_samples)],\n",
        "        'tactic': [random.choice(tactics) for _ in range(num_samples)],\n",
        "        'capability_1': np.random.rand(num_samples) * 0.9 + 0.1,  # Coverage between 0.1 and 1.0\n",
        "        'capability_2': np.random.rand(num_samples) * 0.9 + 0.1,\n",
        "        'capability_3': np.random.rand(num_samples) * 0.9 + 0.1,\n",
        "        'capability_4': np.random.rand(num_samples) * 0.9 + 0.1,\n",
        "        'capability_5': np.random.rand(num_samples) * 0.9 + 0.1,\n",
        "        'prevalence': np.random.rand(num_samples),  # How common the threat action is\n",
        "        'manoeuvrability': np.random.rand(num_samples),  # How easily the threat can maneuver\n",
        "        'detection_score': np.random.rand(num_samples),  # Score for detection capabilities\n",
        "        'protection_score': np.random.rand(num_samples),  # Score for protection capabilities\n",
        "        'response_score': np.random.rand(num_samples),  # Score for response capabilities\n",
        "    }\n",
        "    return pd.DataFrame(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bCcTgWZj8au"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess the dataset\n",
        "def preprocess_data(df):\n",
        "    # Normalize numerical features\n",
        "    scaler = MinMaxScaler()\n",
        "    numerical_features = df[['capability_1', 'capability_2', 'capability_3', 'capability_4', 'capability_5',\n",
        "                             'prevalence', 'manoeuvrability', 'detection_score',\n",
        "                             'protection_score', 'response_score']]\n",
        "    normalized_features = scaler.fit_transform(numerical_features)\n",
        "\n",
        "    # One-hot encode categorical variables\n",
        "    encoder = OneHotEncoder(sparse_output=False)\n",
        "    threat_actions_encoded = encoder.fit_transform(df[['threat_action', 'tactic']])\n",
        "\n",
        "    # Combine normalized and encoded features\n",
        "    processed_data = np.hstack((normalized_features, threat_actions_encoded))\n",
        "    return processed_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovIHNIQyif1I"
      },
      "outputs": [],
      "source": [
        "# DQN Model\n",
        "def build_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(24, input_dim=input_shape, activation='relu'))\n",
        "    model.add(Dense(24, activation='relu'))\n",
        "    model.add(Dense(ACTION_SIZE, activation='linear'))\n",
        "    model.compile(loss='mse', optimizer=Adam(learning_rate=LEARNING_RATE))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wy1Jnowij9w"
      },
      "outputs": [],
      "source": [
        "# Experience Replay\n",
        "class ReplayBuffer:\n",
        "    def __init__(self):\n",
        "        self.buffer = deque(maxlen=2000)\n",
        "\n",
        "    def add(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.buffer, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6txj1wkimzS"
      },
      "outputs": [],
      "source": [
        "# DQN Agent\n",
        "class DQNAgent:\n",
        "    def __init__(self, input_shape):\n",
        "        self.model = build_model(input_shape)\n",
        "        self.memory = ReplayBuffer()\n",
        "        self.epsilon = EPSILON\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(ACTION_SIZE)  # Explore\n",
        "        act_values = self.model.predict(state)\n",
        "        return np.argmax(act_values[0])  # Exploit\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.memory.buffer) < BATCH_SIZE:\n",
        "            return\n",
        "        minibatch = self.memory.sample(BATCH_SIZE)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = reward\n",
        "            if not done:\n",
        "                target += GAMMA * np.amax(self.model.predict(next_state)[0])\n",
        "            target_f = self.model.predict(state)\n",
        "            target_f[0][action] = target\n",
        "            self.model.fit(state, target_f, epochs=1, verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "61Nwrl8Oiqhq",
        "outputId": "df3ad991-6c4f-44b4-d300-dc947245a9b5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
         
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step\n",
            
          ]
        }
      ],
      "source": [
        "\n",
        "dataset = create_mitre_attck_dataset()\n",
        "processed_data = preprocess_data(dataset)\n",
        "\n",
        "agent = DQNAgent(input_shape=processed_data.shape[1])\n",
        "for e in range(EPISODES):\n",
        "    state = processed_data[np.random.randint(0, len(processed_data))].reshape(1, -1)  # Random initial state\n",
        "    for time in range(500):  # Max steps per episode\n",
        "        action = agent.act(state)\n",
        "        # Simulate environment response (replace with actual environment logic)\n",
        "        next_state = processed_data[np.random.randint(0, len(processed_data))].reshape(1, -1)  # Random next state\n",
        "        reward = np.random.rand()  # Replace with actual reward logic\n",
        "        done = False  # Replace with actual done condition\n",
        "\n",
        "        # Store experience in replay buffer\n",
        "        agent.memory.add((state, action, reward, next_state, done))\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            print(f\"Episode {e + 1}/{EPISODES}, score: {time}, epsilon: {agent.epsilon:.2}\")\n",
        "            break\n",
        "\n",
        "        # Train the agent\n",
        "        agent.replay()\n",
        "\n",
        "    # Update exploration rate\n",
        "    if agent.epsilon > EPSILON_MIN:\n",
        "        agent.epsilon *= EPSILON_DECAY"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPuYW+6CYZBbao1H0+KoK1S",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
